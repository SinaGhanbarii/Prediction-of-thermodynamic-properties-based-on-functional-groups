{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12191182,"sourceType":"datasetVersion","datasetId":7678913},{"sourceId":12351506,"sourceType":"datasetVersion","datasetId":7786908}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install RDKit for offline use\n!pip install /kaggle/input/rdkit-install-whl/rdkit_wheel/rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:51:04.274337Z","iopub.execute_input":"2025-07-02T12:51:04.274518Z","iopub.status.idle":"2025-07-02T12:51:09.638862Z","shell.execute_reply.started":"2025-07-02T12:51:04.274495Z","shell.execute_reply":"2025-07-02T12:51:09.638196Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rdkit-install-whl/rdkit_wheel/rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi==2022.9.5) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi==2022.9.5) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi==2022.9.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi==2022.9.5) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit-pypi==2022.9.5) (2024.2.0)\nInstalling collected packages: rdkit-pypi\nSuccessfully installed rdkit-pypi-2022.9.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\n\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, rdMolDescriptors, Descriptors\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom scipy.stats import uniform, randint\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:51:13.071962Z","iopub.execute_input":"2025-07-02T12:51:13.072519Z","iopub.status.idle":"2025-07-02T12:51:16.624724Z","shell.execute_reply.started":"2025-07-02T12:51:13.072483Z","shell.execute_reply":"2025-07-02T12:51:16.624170Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 2. Helper functions\ndef compute_descriptors(smiles_series):\n    \"\"\"Compute 3D inertia, TPSA, MR, and Gasteiger charge stats.\"\"\"\n    cols = ('inertia_sum','inertia_ratio','TPSA','MR','chg_mean','chg_std')\n    records = []\n    for smi in smiles_series:\n        m = Chem.AddHs(Chem.MolFromSmiles(smi))\n        if AllChem.EmbedMolecule(m, randomSeed=42) != 0:\n            vals = [np.nan]*6\n        else:\n            AllChem.UFFOptimizeMolecule(m, confId=0)\n            I1 = rdMolDescriptors.CalcPMI1(m)\n            I2 = rdMolDescriptors.CalcPMI2(m)\n            I3 = rdMolDescriptors.CalcPMI3(m)\n            vals = [\n                I1+I2+I3,\n                I1/(I2+1e-6),\n                rdMolDescriptors.CalcTPSA(m),\n                Descriptors.MolMR(m)\n            ]\n            AllChem.ComputeGasteigerCharges(m)\n            ch = [float(a.GetProp('_GasteigerCharge')) for a in m.GetAtoms()]\n            vals += [np.mean(ch), np.std(ch)]\n        records.append(dict(zip(cols, vals)))\n    return pd.DataFrame.from_records(records)\n\n\n# --- Utility function to create grid from random best params\ndef create_grid_from_best(best_params, steps=3, percent=0.3):\n    grid = {}\n    for k, v in best_params.items():\n        if isinstance(v, (int, float)):\n            low = v * (1 - percent)\n            high = v * (1 + percent)\n            if isinstance(v, int):\n                grid[k] = list(set([int(round(x)) for x in np.linspace(low, high, steps)]))\n            else:\n                grid[k] = np.linspace(low, high, steps)\n        else:\n            grid[k] = [v]\n    return grid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:51:21.024328Z","iopub.execute_input":"2025-07-02T12:51:21.025161Z","iopub.status.idle":"2025-07-02T12:51:21.032644Z","shell.execute_reply.started":"2025-07-02T12:51:21.025138Z","shell.execute_reply":"2025-07-02T12:51:21.031958Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 3. Load & prepare data\ndf = pd.read_csv('/kaggle/input/dataset-9may/dataset_9May.csv')\nsmiles = df.pop('Molecule')\n\n# Compute and append descriptors\ndesc_df = compute_descriptors(smiles)\ndf = pd.concat([df, desc_df], axis=1)\n\n# Define targets\ntargets = ['gap','mu','alpha','homo','lumo','r2','zpve','U0','U','H','G','Cv']\n\n# Define features â€” **EXCLUDE all targets explicitly!**\nfeatures = [c for c in df.columns if c not in targets + ['Molecule']]\n\n# Rebuild feature matrix without leakage\nX = df[features].fillna(0)\nY = df[targets]\n\n# Sanity check â€” should print empty set\nprint(\"Targets in features (should be empty):\", set(targets).intersection(set(X.columns)))\n\n# Train/test split stratified by molecule size\natoms = [c for c in df.columns if len(c)==1 and c.isupper()]\nsize = df[atoms].sum(axis=1)\nX_tr, X_te, Y_tr, Y_te = train_test_split(\n    X, Y, test_size=0.2, random_state=42,\n    stratify=pd.qcut(size, 5)\n)\n\n# Log transform for mu and r2 - training and test\nY_tr[['mu', 'r2']] = np.log1p(Y_tr[['mu', 'r2']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:51:24.689694Z","iopub.execute_input":"2025-07-02T12:51:24.690459Z","iopub.status.idle":"2025-07-02T12:53:41.168116Z","shell.execute_reply.started":"2025-07-02T12:51:24.690435Z","shell.execute_reply":"2025-07-02T12:53:41.167147Z"}},"outputs":[{"name":"stdout","text":"Targets in features (should be empty): set()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Preprocessing pipeline\npre = Pipeline([\n    ('scale', StandardScaler()),\n    ('var', VarianceThreshold(1e-5))\n])\n\n\n# --- Store predictions and models\npreds = pd.DataFrame(index=X_te.index, columns=Y_tr.columns)\nbest_models = {}\n\n# --- Ridge for U0, U, H, G\npipe_ridge = Pipeline([('pre', pre), ('model', MultiOutputRegressor(Ridge()))])\npg_ridge = {'model__estimator__alpha': [0.001, 0.01, 0.1, 1, 10]}\nbest_models['energies'] = GridSearchCV(\n    pipe_ridge, pg_ridge,\n    scoring='neg_root_mean_squared_error',\n    cv=5, n_jobs=-1\n).fit(X_tr, Y_tr[['U0', 'U', 'H', 'G']])\npreds[['U0', 'U', 'H', 'G']] = best_models['energies'].predict(X_te)\n\n# --- Settings for random search\npipe_rf = Pipeline([('pre', pre), ('model', RandomForestRegressor(n_jobs=-1, random_state=42))])\npd_rf = {\n    'model__n_estimators': randint(100, 600),\n    'model__max_depth': randint(5, 20),\n    'model__min_samples_leaf': randint(1, 10),\n    'model__max_features': uniform(0.3, 0.7)\n}\n\npipe_hgbr = Pipeline([('pre', pre), ('model', HistGradientBoostingRegressor(random_state=42))])\npd_hgbr = {\n    'model__max_iter': randint(300, 800),\n    'model__learning_rate': uniform(0.01, 0.15),\n    'model__max_depth': randint(4, 12),\n    'model__l2_regularization': uniform(0, 0.5)\n}\n\n# --- RF targets\nfor t in ['gap', 'mu', 'homo', 'lumo', 'r2']:\n    print(f\"\\nğŸ” {t.upper()} - Random Forest\")\n    rs = RandomizedSearchCV(pipe_rf, pd_rf, n_iter=20, cv=3, scoring='neg_root_mean_squared_error',\n                            n_jobs=-1, random_state=42)\n    rs.fit(X_tr, Y_tr[t])\n    grid = create_grid_from_best(rs.best_params_, steps=3)\n    gs = GridSearchCV(pipe_rf, grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    gs.fit(X_tr, Y_tr[t])\n    best_models[t] = gs\n    pred = gs.predict(X_te)\n    preds[t] = np.expm1(pred) if t in ['mu', 'r2'] else pred\n\n# --- HGBR targets\nfor t in ['alpha', 'zpve', 'Cv']:\n    print(f\"\\nğŸ” {t.upper()} - HGBR\")\n    rs = RandomizedSearchCV(pipe_hgbr, pd_hgbr, n_iter=30, cv=3, scoring='neg_root_mean_squared_error',\n                            n_jobs=-1, random_state=42)\n    rs.fit(X_tr, Y_tr[t])\n    grid = create_grid_from_best(rs.best_params_, steps=3)\n    gs = GridSearchCV(pipe_hgbr, grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    gs.fit(X_tr, Y_tr[t])\n    best_models[t] = gs\n    preds[t] = gs.predict(X_te)\n\n# --- Evaluation\nfor t in Y_tr.columns:\n    rmse = np.sqrt(mean_squared_error(Y_te[t], preds[t]))\n    mae = mean_absolute_error(Y_te[t], preds[t])\n    r2val = r2_score(Y_te[t], preds[t])\n    print(f\"{t:>6}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2val:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:53:55.049044Z","iopub.execute_input":"2025-07-02T12:53:55.049327Z","iopub.status.idle":"2025-07-02T13:57:11.149470Z","shell.execute_reply.started":"2025-07-02T12:53:55.049305Z","shell.execute_reply":"2025-07-02T13:57:11.148704Z"}},"outputs":[{"name":"stdout","text":"\nğŸ” GAP - Random Forest\n\nğŸ” MU - Random Forest\n\nğŸ” HOMO - Random Forest\n\nğŸ” LUMO - Random Forest\n\nğŸ” R2 - Random Forest\n\nğŸ” ALPHA - HGBR\n\nğŸ” ZPVE - HGBR\n\nğŸ” CV - HGBR\n   gap: RMSE=0.0184, MAE=0.0126, R2=0.8430\n    mu: RMSE=1.0061, MAE=0.6728, R2=0.5742\n alpha: RMSE=1.3001, MAE=0.8174, R2=0.9633\n  homo: RMSE=0.0102, MAE=0.0070, R2=0.8114\n  lumo: RMSE=0.0161, MAE=0.0108, R2=0.8737\n    r2: RMSE=79.2523, MAE=48.1949, R2=0.8945\n  zpve: RMSE=0.0049, MAE=0.0031, R2=0.9752\n    U0: RMSE=0.7115, MAE=0.5284, R2=0.9994\n     U: RMSE=0.7110, MAE=0.5281, R2=0.9994\n     H: RMSE=0.7110, MAE=0.5281, R2=0.9994\n     G: RMSE=0.7119, MAE=0.5287, R2=0.9994\n    Cv: RMSE=0.9643, MAE=0.7251, R2=0.9209\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Store predictions and models\npreds = pd.DataFrame(index=X_te.index, columns=Y_tr.columns)\nbest_models = {}\n\n# --- Ridge for U0, U, H, G\npipe_ridge = Pipeline([('pre', pre), ('model', MultiOutputRegressor(Ridge()))])\npg_ridge = {'model__estimator__alpha': [0.001, 0.01, 0.1, 1, 10]}\n\ngs_ridge = GridSearchCV(\n    pipe_ridge, pg_ridge,\n    scoring='neg_root_mean_squared_error',\n    cv=5, n_jobs=-1\n)\ngs_ridge.fit(X_tr, Y_tr[['U0', 'U', 'H', 'G']])\n\n# Show best parameters\nprint(f\"\\nğŸ” Ridge (U0, U, H, G) best params: {gs_ridge.best_params_}\")\n\nbest_models['energies'] = gs_ridge\npreds[['U0', 'U', 'H', 'G']] = gs_ridge.predict(X_te)\n\n\n# --- Settings for random search\npipe_rf = Pipeline([('pre', pre), ('model', RandomForestRegressor(n_jobs=-1, random_state=42))])\npd_rf = {\n    'model__n_estimators': randint(100, 600),\n    'model__max_depth': randint(5, 20),\n    'model__min_samples_leaf': randint(1, 10),\n    'model__max_features': uniform(0.3, 0.7)\n}\n\npipe_hgbr = Pipeline([('pre', pre), ('model', HistGradientBoostingRegressor(random_state=42))])\npd_hgbr = {\n    'model__max_iter': randint(300, 800),\n    'model__learning_rate': uniform(0.01, 0.15),\n    'model__max_depth': randint(4, 12),\n    'model__l2_regularization': uniform(0, 0.5)\n}\n\n# --- RF targets\nfor t in ['gap', 'mu', 'homo', 'lumo', 'r2']:\n    print(f\"\\nğŸ” {t.upper()} - Random Forest\")\n    rs = RandomizedSearchCV(pipe_rf, pd_rf, n_iter=20, cv=3, scoring='neg_root_mean_squared_error',\n                            n_jobs=-1, random_state=42)\n    rs.fit(X_tr, Y_tr[t])\n    print(f\"  ğŸ”§ Best (Random): {rs.best_params_}\")\n    \n    grid = create_grid_from_best(rs.best_params_, steps=3)\n    gs = GridSearchCV(pipe_rf, grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    gs.fit(X_tr, Y_tr[t])\n    best_models[t] = gs\n\n    pred = gs.predict(X_te)\n    preds[t] = np.expm1(pred) if t in ['mu', 'r2'] else pred\n\n    print(f\"  ğŸ¯ Best (Grid): {gs.best_params_}\")\n\n# --- HGBR targets\nfor t in ['alpha', 'zpve', 'Cv']:\n    print(f\"\\nğŸ” {t.upper()} - HGBR\")\n    rs = RandomizedSearchCV(pipe_hgbr, pd_hgbr, n_iter=30, cv=3, scoring='neg_root_mean_squared_error',\n                            n_jobs=-1, random_state=42)\n    rs.fit(X_tr, Y_tr[t])\n    print(f\"  ğŸ”§ Best (Random): {rs.best_params_}\")\n    \n    grid = create_grid_from_best(rs.best_params_, steps=3)\n    gs = GridSearchCV(pipe_hgbr, grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n    gs.fit(X_tr, Y_tr[t])\n    best_models[t] = gs\n    preds[t] = gs.predict(X_te)\n\n    print(f\"  ğŸ¯ Best (Grid): {gs.best_params_}\")\n\n# --- Evaluation\nfor t in Y_tr.columns:\n    rmse = np.sqrt(mean_squared_error(Y_te[t], preds[t]))\n    mae = mean_absolute_error(Y_te[t], preds[t])\n    r2val = r2_score(Y_te[t], preds[t])\n    print(f\"{t:>6}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2val:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:10.083240Z","iopub.execute_input":"2025-07-02T15:08:10.083531Z","iopub.status.idle":"2025-07-02T16:07:26.689138Z","shell.execute_reply.started":"2025-07-02T15:08:10.083510Z","shell.execute_reply":"2025-07-02T16:07:26.688496Z"}},"outputs":[{"name":"stdout","text":"\nğŸ” Ridge (U0, U, H, G) best params: {'model__estimator__alpha': 0.01}\n\nğŸ” GAP - Random Forest\n  ğŸ”§ Best (Random): {'model__max_depth': 17, 'model__max_features': 0.8832364382153151, 'model__min_samples_leaf': 3, 'model__n_estimators': 305}\n  ğŸ¯ Best (Grid): {'model__max_depth': 25, 'model__max_features': 0.88324, 'model__min_samples_leaf': 1, 'model__n_estimators': 457}\n\nğŸ” MU - Random Forest\n  ğŸ”§ Best (Random): {'model__max_depth': 17, 'model__max_features': 0.4899443222417271, 'model__min_samples_leaf': 5, 'model__n_estimators': 379}\n  ğŸ¯ Best (Grid): {'model__max_depth': 25, 'model__max_features': 0.48994, 'model__min_samples_leaf': 4, 'model__n_estimators': 568}\n\nğŸ” HOMO - Random Forest\n  ğŸ”§ Best (Random): {'model__max_depth': 17, 'model__max_features': 0.8832364382153151, 'model__min_samples_leaf': 3, 'model__n_estimators': 305}\n  ğŸ¯ Best (Grid): {'model__max_depth': 25, 'model__max_features': 0.44162, 'model__min_samples_leaf': 1, 'model__n_estimators': 457}\n\nğŸ” LUMO - Random Forest\n  ğŸ”§ Best (Random): {'model__max_depth': 17, 'model__max_features': 0.8832364382153151, 'model__min_samples_leaf': 3, 'model__n_estimators': 305}\n  ğŸ¯ Best (Grid): {'model__max_depth': 25, 'model__max_features': 0.44162, 'model__min_samples_leaf': 1, 'model__n_estimators': 457}\n\nğŸ” R2 - Random Forest\n  ğŸ”§ Best (Random): {'model__max_depth': 17, 'model__max_features': 0.4899443222417271, 'model__min_samples_leaf': 5, 'model__n_estimators': 379}\n  ğŸ¯ Best (Grid): {'model__max_depth': 16, 'model__max_features': 0.48994, 'model__min_samples_leaf': 2, 'model__n_estimators': 568}\n\nğŸ” ALPHA - HGBR\n  ğŸ”§ Best (Random): {'model__l2_regularization': 0.37366005506869043, 'model__learning_rate': 0.09095381985836196, 'model__max_depth': 11, 'model__max_iter': 709}\n  ğŸ¯ Best (Grid): {'model__l2_regularization': 0.37366, 'model__learning_rate': 0.09095, 'model__max_depth': 5, 'model__max_iter': 1063}\n\nğŸ” ZPVE - HGBR\n  ğŸ”§ Best (Random): {'model__l2_regularization': 0.02904180608409973, 'model__learning_rate': 0.13992642186624027, 'model__max_depth': 7, 'model__max_iter': 659}\n  ğŸ¯ Best (Grid): {'model__l2_regularization': 0.02904, 'model__learning_rate': 0.13993, 'model__max_depth': 6, 'model__max_iter': 988}\n\nğŸ” CV - HGBR\n  ğŸ”§ Best (Random): {'model__l2_regularization': 0.2117007403531848, 'model__learning_rate': 0.06923222772633546, 'model__max_depth': 11, 'model__max_iter': 771}\n  ğŸ¯ Best (Grid): {'model__l2_regularization': 0.2117, 'model__learning_rate': 0.06923, 'model__max_depth': 16, 'model__max_iter': 1156}\n   gap: RMSE=0.0182, MAE=0.0123, R2=0.8470\n    mu: RMSE=1.0061, MAE=0.6727, R2=0.5741\n alpha: RMSE=1.3029, MAE=0.8338, R2=0.9631\n  homo: RMSE=0.0101, MAE=0.0070, R2=0.8151\n  lumo: RMSE=0.0159, MAE=0.0107, R2=0.8772\n    r2: RMSE=78.6170, MAE=47.7375, R2=0.8962\n  zpve: RMSE=0.0049, MAE=0.0032, R2=0.9754\n    U0: RMSE=0.7115, MAE=0.5284, R2=0.9994\n     U: RMSE=0.7110, MAE=0.5281, R2=0.9994\n     H: RMSE=0.7110, MAE=0.5281, R2=0.9994\n     G: RMSE=0.7119, MAE=0.5287, R2=0.9994\n    Cv: RMSE=0.9612, MAE=0.7187, R2=0.9214\n","output_type":"stream"}],"execution_count":9}]}