{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmark Before Feature Engineering\n",
        "\n",
        "This notebook performs a comprehensive benchmark of machine learning models for predicting thermodynamic properties of molecules **before** applying feature engineering techniques. The goal is to establish baseline performance metrics that can be compared against models trained on engineered features.\n",
        "\n",
        "## Workflow Summary\n",
        "1. **Data Loading & Preparation**: Load molecular dataset and prepare features/targets\n",
        "2. **Data Splitting**: Stratified train/test split based on molecular size\n",
        "3. **Preprocessing Pipeline**: Standardization and variance filtering\n",
        "4. **Model Selection & Tuning**: Hyperparameter optimization for multiple regressors\n",
        "5. **Evaluation**: Performance assessment using RMSE, MAE, and R¬≤\n",
        "\n",
        "## Regressor Selection Rationale\n",
        "\n",
        "| **Model** | **Why Chosen** | **Key Advantages** | **Best For** |\n",
        "|-----------|----------------|-------------------|--------------|\n",
        "| **Histogram-based Gradient Boosting (HGBR)** | Excellent performance on tabular data with mixed feature types | ‚Ä¢ Handles missing values natively<br>‚Ä¢ Fast training on large datasets<br>‚Ä¢ Built-in categorical feature support<br>‚Ä¢ Robust to outliers | Complex non-linear relationships in molecular properties |\n",
        "| **Random Forest (RF)** | Proven ensemble method with strong baseline performance | ‚Ä¢ Handles feature interactions well<br>‚Ä¢ Provides feature importance metrics<br>‚Ä¢ Robust to overfitting<br>‚Ä¢ Works well with high-dimensional data | Capturing complex feature interactions without extensive tuning |\n",
        "| **Ridge Regression** | Linear baseline with regularization | ‚Ä¢ Fast training and prediction<br>‚Ä¢ Interpretable coefficients<br>‚Ä¢ Handles multicollinearity<br>‚Ä¢ Good baseline for linear relationships | Establishing linear baseline and identifying targets with linear trends |\n",
        "\n",
        "\n",
        "### **Excluded Models**\n",
        "- **SVR**: Computationally expensive for large datasets and hyperparameter tuning\n",
        "- **Neural Networks**: Reserved for separate analysis in dedicated notebooks\n",
        "- **XGBoost**: HGBR provides similar performance with faster training\n",
        "\n",
        "This combination ensures we capture different types of relationships in the data while maintaining computational efficiency.\n",
        "\n",
        "## Key Preprocessing Steps\n",
        "- **Standardization**: Essential for Ridge regression and consistent scaling\n",
        "- **Variance Filtering**: Removes near-constant features (threshold: 1e-5)\n",
        "- **Stratified Splitting**: Ensures balanced distribution of molecular sizes\n",
        "- **Log Transformation**: Applied to `mu` and `r2` due to their skewed distributions\n",
        "\n",
        "## Expected Outcomes\n",
        "This benchmark will establish baseline performance metrics that help us:\n",
        "1. Identify which properties are inherently easier/harder to predict\n",
        "2. Understand model-specific strengths for different property types\n",
        "3. Provide comparison baseline for feature engineering improvements\n",
        "4. Guide selection of best-performing models for further optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQNpsKrlrUFf"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "from scipy.stats import uniform, randint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-2Bnr5ksa0h"
      },
      "outputs": [],
      "source": [
        "# Load & prepare data\n",
        "df = pd.read_csv('dataset_9May.csv')\n",
        "smiles = df.pop('Molecule')  # Optional: drop SMILES string\n",
        "\n",
        "# Define targets\n",
        "targets = ['gap','mu','alpha','homo','lumo','r2','zpve','U0','U','H','G','Cv']\n",
        "features = [c for c in df.columns if c not in targets]\n",
        "\n",
        "X = df[features].fillna(0)\n",
        "Y = df[targets]\n",
        "\n",
        "# Train/test split stratified by molecule size\n",
        "atoms = [c for c in df.columns if len(c)==1 and c.isupper()]\n",
        "size = df[atoms].sum(axis=1)\n",
        "X_tr, X_te, Y_tr, Y_te = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42,\n",
        "    stratify=pd.qcut(size, 5)\n",
        ")\n",
        "\n",
        "# Log transform for mu and r2 - training and test\n",
        "Y_tr[['mu', 'r2']] = np.log1p(Y_tr[['mu', 'r2']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg8On0Zbsltk"
      },
      "outputs": [],
      "source": [
        "# --- Define preprocessing pipeline\n",
        "pre = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('var', VarianceThreshold(1e-5))\n",
        "])\n",
        "\n",
        "# --- Define regressor search spaces\n",
        "param_spaces = {\n",
        "    'HGBR': {\n",
        "        'model__max_iter': randint(300, 800),\n",
        "        'model__learning_rate': uniform(0.01, 0.15),\n",
        "        'model__max_depth': randint(3, 10),\n",
        "        'model__l2_regularization': uniform(0, 0.5),\n",
        "        'model__max_leaf_nodes': randint(20, 100)\n",
        "    },\n",
        "    'RF': {\n",
        "        'model__n_estimators': [100, 300, 500],\n",
        "        'model__max_depth': [None, 10, 20],\n",
        "        'model__min_samples_leaf': [1, 5, 10],\n",
        "        'model__max_features': ['sqrt', 0.5, 0.8]\n",
        "    },\n",
        "    'Ridge': {\n",
        "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Define regressors\n",
        "regressors = {\n",
        "    'HGBR': HistGradientBoostingRegressor(random_state=42),\n",
        "    'RF': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    'Ridge': Ridge()\n",
        "}\n",
        "\n",
        "# --- Results container\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5q6ihQpssZk",
        "outputId": "15fcfaa9-215d-4c0c-ece3-e0e312364244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Target: gap\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=0.0302, MAE=0.0236, R2=0.5758\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=0.0303, MAE=0.0236, R2=0.5728\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.0339, MAE=0.0278, R2=0.4679\n",
            "\n",
            "üîç Target: mu\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=1.0697, MAE=0.7335, R2=0.5186\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=1.0671, MAE=0.7332, R2=0.5209\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=1.2644, MAE=0.9210, R2=0.3274\n",
            "\n",
            "üîç Target: alpha\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=2.7304, MAE=2.0240, R2=0.8380\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=2.7395, MAE=2.0179, R2=0.8369\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=2.7268, MAE=2.0226, R2=0.8384\n",
            "\n",
            "üîç Target: homo\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=0.0147, MAE=0.0108, R2=0.6129\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=0.0146, MAE=0.0107, R2=0.6160\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.0160, MAE=0.0120, R2=0.5389\n",
            "\n",
            "üîç Target: lumo\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=0.0260, MAE=0.0200, R2=0.6714\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=0.0258, MAE=0.0197, R2=0.6764\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.0287, MAE=0.0230, R2=0.5981\n",
            "\n",
            "üîç Target: r2\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=224.1464, MAE=155.9047, R2=0.1563\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=224.1905, MAE=155.9286, R2=0.1559\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=223.7268, MAE=155.3517, R2=0.1594\n",
            "\n",
            "üîç Target: zpve\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=0.0177, MAE=0.0141, R2=0.6824\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=0.0177, MAE=0.0140, R2=0.6826\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.0184, MAE=0.0148, R2=0.6554\n",
            "\n",
            "üîç Target: U0\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=1.1241, MAE=0.7968, R2=0.9986\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=1.3253, MAE=0.7876, R2=0.9981\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.9775, MAE=0.7849, R2=0.9989\n",
            "\n",
            "üîç Target: U\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=1.1403, MAE=0.7991, R2=0.9986\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=1.3714, MAE=0.7904, R2=0.9979\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.9768, MAE=0.7844, R2=0.9989\n",
            "\n",
            "üîç Target: H\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=1.1403, MAE=0.7991, R2=0.9986\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=1.3606, MAE=0.7904, R2=0.9979\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.9768, MAE=0.7844, R2=0.9989\n",
            "\n",
            "üîç Target: G\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=1.4646, MAE=0.8238, R2=0.9976\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=1.3590, MAE=0.7947, R2=0.9979\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=0.9782, MAE=0.7856, R2=0.9989\n",
            "\n",
            "üîç Target: Cv\n",
            "  ‚û§ Tuning HGBR...\n",
            "    ‚úì HGBR: RMSE=2.4653, MAE=1.9815, R2=0.4831\n",
            "  ‚û§ Tuning RF...\n",
            "    ‚úì RF: RMSE=2.4520, MAE=1.9620, R2=0.4887\n",
            "  ‚û§ Tuning Ridge...\n",
            "    ‚úì Ridge: RMSE=2.4976, MAE=2.0081, R2=0.4695\n"
          ]
        }
      ],
      "source": [
        "# --- Loop over targets and regressors\n",
        "for target in targets:\n",
        "    print(f\"\\nüîç Target: {target}\")\n",
        "    results[target] = {}\n",
        "    for name, model in regressors.items():\n",
        "        print(f\"  ‚û§ Tuning {name}...\")\n",
        "        pipe = Pipeline([\n",
        "            ('pre', pre),\n",
        "            ('model', model)\n",
        "        ])\n",
        "        # Use RandomizedSearchCV for hyperparameter tuning with less computational cost\n",
        "        tuned_model = RandomizedSearchCV(\n",
        "            pipe, param_spaces[name],\n",
        "            scoring='neg_root_mean_squared_error',\n",
        "            cv=3,\n",
        "            n_iter=20,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "        # Fit the model on training data\n",
        "        tuned_model.fit(X_tr, Y_tr[target])\n",
        "        # Get the best pipeline\n",
        "        best_pipe = tuned_model.best_estimator_\n",
        "        # Predict on test data\n",
        "        preds_ = best_pipe.predict(X_te)\n",
        "\n",
        "        # Inverse transform for log-transformed targets\n",
        "        if target in ['mu', 'r2']:\n",
        "            preds_ = np.expm1(preds_)\n",
        "            true_vals = np.expm1(Y_te[target])\n",
        "        else:\n",
        "            true_vals = Y_te[target]\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(Y_te[target], preds_))\n",
        "        mae = mean_absolute_error(Y_te[target], preds_)\n",
        "        r2 = r2_score(Y_te[target], preds_)\n",
        "\n",
        "        results[target][name] = {\n",
        "            'RMSE': rmse, 'MAE': mae, 'R2': r2\n",
        "        }\n",
        "        print(f\"    ‚úì {name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
        "\n",
        "# --- Convert to DataFrame\n",
        "benchmark_df = pd.concat({\n",
        "    t: pd.DataFrame(v).T for t, v in results.items()\n",
        "}, axis=0)\n",
        "benchmark_df.index.names = ['Target', 'Model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZWJyGtA7inU",
        "outputId": "c62c0c95-6fdb-4c5d-a85a-5f348d07ab47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    RMSE         MAE        R2\n",
            "Target Model                                  \n",
            "gap    HGBR     0.030234    0.023564  0.575822\n",
            "       RF       0.030340    0.023555  0.572846\n",
            "       Ridge    0.033862    0.027766  0.467891\n",
            "mu     HGBR     1.069721    0.733469  0.518576\n",
            "       RF       1.067138    0.733215  0.520898\n",
            "       Ridge    1.264406    0.921028  0.327395\n",
            "alpha  HGBR     2.730353    2.023988  0.837992\n",
            "       RF       2.739537    2.017891  0.836901\n",
            "       Ridge    2.726814    2.022601  0.838412\n",
            "homo   HGBR     0.014654    0.010835  0.612929\n",
            "       RF       0.014595    0.010734  0.616044\n",
            "       Ridge    0.015994    0.011990  0.538877\n",
            "lumo   HGBR     0.025992    0.019967  0.671368\n",
            "       RF       0.025794    0.019707  0.676351\n",
            "       Ridge    0.028744    0.022960  0.598068\n",
            "r2     HGBR   224.146351  155.904724  0.156276\n",
            "       RF     224.190535  155.928552  0.155944\n",
            "       Ridge  223.726804  155.351656  0.159432\n",
            "zpve   HGBR     0.017705    0.014066  0.682374\n",
            "       RF       0.017699    0.014003  0.682565\n",
            "       Ridge    0.018442    0.014779  0.655373\n",
            "U0     HGBR     1.124104    0.796777  0.998597\n",
            "       RF       1.325283    0.787599  0.998050\n",
            "       Ridge    0.977457    0.784915  0.998939\n",
            "U      HGBR     1.140322    0.799132  0.998556\n",
            "       RF       1.371433    0.790378  0.997912\n",
            "       Ridge    0.976791    0.784356  0.998941\n",
            "H      HGBR     1.140322    0.799132  0.998556\n",
            "       RF       1.360577    0.790409  0.997945\n",
            "       Ridge    0.976791    0.784356  0.998941\n",
            "G      HGBR     1.464574    0.823845  0.997619\n",
            "       RF       1.359009    0.794700  0.997950\n",
            "       Ridge    0.978250    0.785592  0.998938\n",
            "Cv     HGBR     2.465288    1.981486  0.483138\n",
            "       RF       2.452043    1.961959  0.488677\n",
            "       Ridge    2.497557    2.008112  0.469518\n"
          ]
        }
      ],
      "source": [
        "# --- Display benchmark results\n",
        "print(benchmark_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
